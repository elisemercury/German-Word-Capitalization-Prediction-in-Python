{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import *\n",
    "import string\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# german stopwords\n",
    "de_stop_words = stopwords.words('german')\n",
    "de_stop_words_cap = [x.capitalize() for x in de_stop_words]\n",
    "\n",
    "# creating a stopword dictionary for simplifying replacement\n",
    "de_stop_words_dict = {de_stop_words_cap[i]: de_stop_words[i] for i in range(len(de_stop_words_cap))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for replacing elements in a list matching the given dictionary keys\n",
    "def replace_matched_items(word_list, dictionary):\n",
    "    for lst in word_list:\n",
    "        for ind, item in enumerate(lst):\n",
    "            lst[ind] = dictionary.get(item, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-27b71eadc4ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# remove digits, and the following words f.e. 1. August 2020 - 1. and 2020 are removed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mclean_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\S*\\d\\S*\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclean_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# tokenize in list of articles with sentences as list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-27b71eadc4ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# remove digits, and the following words f.e. 1. August 2020 - 1. and 2020 are removed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mclean_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\S*\\d\\S*\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclean_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# tokenize in list of articles with sentences as list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = []\n",
    "dir_list = os.listdir(os.getcwd() + \"\\\\extracted\\\\\")\n",
    "file_list = []\n",
    "l = 0\n",
    "file = 0\n",
    "ticker = 0\n",
    "max_batch = 2\n",
    "\n",
    "while ticker < 58:\n",
    "    \n",
    "    while l < max_batch:\n",
    "        for filename in os.listdir(os.getcwd() + \"\\\\extracted\\\\\" + dir_list[file]):\n",
    "            with open(os.path.join(os.getcwd() + \"\\\\extracted\\\\\" + dir_list[file] + \"\\\\\" + filename), 'rt', encoding=\"utf-8\") as f:\n",
    "                train.append(f.read())\n",
    "        file += 1\n",
    "        l += 1\n",
    "        \n",
    "    clean_train = [x.encode('utf-8').decode('utf-8') for x in train]\n",
    "\n",
    "    # cleaning\n",
    "    \n",
    "    # removing double space, chars and line breaks\n",
    "    clean_train = [((x.replace(\"\\xa0\", \" \")).replace(\"\\n\", \" \")).replace(\"  \", \" \") for x in clean_train] \n",
    "\n",
    "    # remove <doc> tag, link and article id in beginning\n",
    "    clean_train = [re.sub(r'<.+?> ', '', x) for x in clean_train]\n",
    "  \n",
    "    # remove digits, and the following words f.e. 1. August 2020 - 1. and 2020 are removed\n",
    "    clean_train = [re.sub(\"\\S*\\d\\S*\", '', x).strip() for x in clean_train]\n",
    "\n",
    "    # tokenize in list of articles with sentences as list\n",
    "    clean_train = [sent_tokenize(x) for x in clean_train]\n",
    "\n",
    "    # flatten list = one big list with multiple sentences - no article recognition anymore - usefull (?)\n",
    "    clean_train = [item for sublist in clean_train for item in sublist]\n",
    "\n",
    "    # remove punctuation\n",
    "    clean_train = [re.sub(r'[^\\w\\s]', '', x).strip() for x in clean_train]\n",
    "\n",
    "    # tokenize all words in sentences = on bis list, with list of articles with tokenized words\n",
    "    clean_train = [word_tokenize(x) for x in clean_train]    \n",
    "\n",
    "    replace_matched_items(clean_train, de_stop_words_dict)\n",
    "\n",
    "    #training\n",
    "    \n",
    "    if ticker < max_batch:\n",
    "        model = Word2Vec(clean_train)\n",
    "        model.save(\"w2v_model.bin\")\n",
    "        del model\n",
    "        print(\"Model created at ticker \", ticker) \n",
    "        train = []\n",
    "        ticker += max_batch\n",
    "        l = 0\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec.load(\"w2v_model.bin\")\n",
    "        model.train(clean_train, total_examples=(len(clean_train)), epochs=model.epochs)\n",
    "        model.save(\"w2v_model.bin\")\n",
    "        del model\n",
    "        print(\"Model updated at ticker \", ticker) \n",
    "        train = []\n",
    "        ticker += max_batch\n",
    "        l = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
