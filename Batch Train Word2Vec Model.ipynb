{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import *\n",
    "import string\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# german stopwords\n",
    "de_stop_words = stopwords.words('german')\n",
    "de_stop_words_cap = [x.capitalize() for x in de_stop_words]\n",
    "\n",
    "# creating a stopword dictionary for simplifying replacement\n",
    "de_stop_words_dict = {de_stop_words_cap[i]: de_stop_words[i] for i in range(len(de_stop_words_cap))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for replacing elements in a list matching the given dictionary keys\n",
    "def replace_matched_items(word_list, dictionary):\n",
    "    for lst in word_list:\n",
    "        for ind, item in enumerate(lst):\n",
    "            lst[ind] = dictionary.get(item, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created at ticker  0\n",
      "Model updated at ticker  2\n",
      "Model updated at ticker  4\n",
      "Model updated at ticker  6\n",
      "Model updated at ticker  8\n",
      "Model updated at ticker  10\n",
      "Model updated at ticker  12\n",
      "Model updated at ticker  14\n",
      "Model updated at ticker  16\n",
      "Model updated at ticker  18\n",
      "Model updated at ticker  20\n",
      "Model updated at ticker  22\n",
      "Model updated at ticker  24\n",
      "Model updated at ticker  26\n",
      "Model updated at ticker  28\n",
      "Model updated at ticker  30\n",
      "Model updated at ticker  32\n",
      "Model updated at ticker  34\n",
      "Model updated at ticker  36\n",
      "Model updated at ticker  38\n",
      "Model updated at ticker  40\n",
      "Model updated at ticker  42\n",
      "Model updated at ticker  44\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "dir_list = os.listdir(os.getcwd() + \"\\\\extracted\\\\\")\n",
    "file_list = []\n",
    "l = 0\n",
    "file = 0\n",
    "ticker = 0\n",
    "max_batch = 2\n",
    "\n",
    "while ticker <= 46:\n",
    "    \n",
    "    while l < max_batch:\n",
    "        for filename in os.listdir(os.getcwd() + \"\\\\extracted\\\\\" + dir_list[file]):\n",
    "            with open(os.path.join(os.getcwd() + \"\\\\extracted\\\\\" + dir_list[file] + \"\\\\\" + filename), 'rt', encoding=\"utf-8\") as f:\n",
    "                train.append(f.read())\n",
    "        file += 1\n",
    "        l += 1\n",
    "        \n",
    "    clean_train = [x.encode('utf-8').decode('utf-8') for x in train]\n",
    "\n",
    "    # cleaning\n",
    "    \n",
    "    # removing double space, chars and line breaks\n",
    "    clean_train = [((x.replace(\"\\xa0\", \" \")).replace(\"\\n\", \" \")).replace(\"  \", \" \") for x in clean_train] \n",
    "\n",
    "    # remove <doc> tag, link and article id in beginning\n",
    "    clean_train = [re.sub(r'<.+?> ', '', x) for x in clean_train]\n",
    "  \n",
    "    # remove digits, and the following words f.e. 1. August 2020 - 1. and 2020 are removed\n",
    "    clean_train = [re.sub(\"\\S*\\d\\S*\", '', x).strip() for x in clean_train]\n",
    "\n",
    "    # tokenize in list of articles with sentences as list\n",
    "    clean_train = [sent_tokenize(x) for x in clean_train]\n",
    "\n",
    "    # flatten list = one big list with multiple sentences - no article recognition anymore - usefull (?)\n",
    "    clean_train = [item for sublist in clean_train for item in sublist]\n",
    "\n",
    "    # remove punctuation\n",
    "    clean_train = [re.sub(r'[^\\w\\s]', '', x).strip() for x in clean_train]\n",
    "\n",
    "    # tokenize all words in sentences = on bis list, with list of articles with tokenized words\n",
    "    clean_train = [word_tokenize(x) for x in clean_train]    \n",
    "\n",
    "    replace_matched_items(clean_train, de_stop_words_dict)\n",
    "\n",
    "    #training\n",
    "    \n",
    "    if ticker < max_batch:\n",
    "        model = Word2Vec(clean_train)\n",
    "        model.save(\"w2v_model_train.bin\")\n",
    "        del model\n",
    "        print(\"Model created at ticker \", ticker) \n",
    "        train = []\n",
    "        ticker += max_batch\n",
    "        l = 0\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec.load(\"w2v_model_train.bin\")\n",
    "        model.train(clean_train, total_examples=(len(clean_train)), epochs=model.epochs)\n",
    "        model.save(\"w2v_model_train.bin\")\n",
    "        del model\n",
    "        print(\"Model updated at ticker \", ticker) \n",
    "        train = []\n",
    "        ticker += max_batch\n",
    "        l = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
