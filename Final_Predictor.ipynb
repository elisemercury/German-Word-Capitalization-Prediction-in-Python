{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Word Capitalization Prediction\n",
    "\n",
    "### *How to create an NLP Machine Learning model for predicting noun capilaization in German?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Prepare Test Data\n",
    "\n",
    "Test data ist located in files in the following directory numbers: **dir_list [ 95 : 118 ]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_test_data(directory_nr, output_list):\n",
    "    dir_list = os.listdir(os.getcwd() + \"\\\\extracted\\\\\")\n",
    "    \n",
    "    for filename in os.listdir(os.getcwd() + \"\\\\extracted\\\\\" + dir_list[directory_nr]):\n",
    "        with open(os.path.join(os.getcwd() + \"\\\\extracted\\\\\" + dir_list[directory_nr] + \"\\\\\" + filename), 'rt', encoding=\"utf-8\") as f:\n",
    "            output_list.append(f.read())\n",
    "\n",
    "    output_list[:] = [x.encode('utf-8').decode('utf-8') for x in output_list]\n",
    "\n",
    "    # cleaning\n",
    "    \n",
    "    # removing double space, chars and line breaks\n",
    "    output_list[:] = [((x.replace(\"\\xa0\", \" \")).replace(\"\\n\", \" \")).replace(\"  \", \" \") for x in output_list] \n",
    "\n",
    "    # remove <doc> tag, link and article id in beginning\n",
    "    output_list[:] = [re.sub(r'<.+?> ', '', x) for x in output_list]\n",
    "    \n",
    "    #return output_list[0]\n",
    "  \n",
    "    # remove digits, and the following words f.e. 1. August 2020 - 1. and 2020 are removed\n",
    "    # output_list = [re.sub(\"\\S*\\d\\S*\", '', x).strip() for x in output_list]\n",
    "    \n",
    "    #return output_list\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_95 = []\n",
    "clean_test_data(95, test_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pazifische Taifunsaison 2015 Die Pazifische Taifunsaison 2015 ist ein andauerndes Wetterereignis, unter dem die sich während des gesamten Kalenderjahres bildenden tropischen Wirbelstürme zusammengefasst sind. Die meisten tropischen Wirbelstürme bilden sich im Pazifischen Ozean westlich der Datumsgre'"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_95[0][0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_95[0].split()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pazifische Taifunsaison 2015 Die Pazifische Taifunsaison 2015 ist ein andauerndes Wetterereignis, unter dem die sich während des gesamten Kalenderjahres bildenden tropischen Wirbelstürme zusammengefasst sind. Die meisten tropischen Wirbelstürme bilden sich im Pazifischen Ozean westlich der Datumsgrenze und nördlich des Äquators jedoch zwischen Mai und November. Solche tropischen Wirbelstürme nennt man Taifune. Die sich östlich des 180. Längengrades bildenden Stürme sind Gegenstand der Pazifischen Hurrikansaison 2015. Innerhalb des nordwestlichen Pazifiks gibt es zwei meteorologische Organisationen, die den Stürmen Namen vergeben. Dies führt dazu, dass derselbe Sturm häufig zwei verschiedene Namen erhält. Die offizielle Bezeichnung durch die Japan Meteorological Agency (JMA) erhält'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = \" \".join(test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Test Data - Method Implementation - Built in Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"w2v_model_train_48.bin\")\n",
    "lst = model.predict_output_word([\"meinem\", \"haus\", \"ein\" ], topn=model.wv.vectors.shape[0]) \n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo Bar\n"
     ]
    }
   ],
   "source": [
    "print(\"Foo\",end=' ')\n",
    "print('Bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = \"ich Habe anna 5. meinem haus ein Garten mit vielen Blumen Wie rosen.\"\n",
    "actual_x = \"Ich habe Anna 5. meinem Haus ein Garten mit vielen Blumen wie Rosen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 3, 24, 20, 33, 43, 615745)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "##hourMinute = datetime.now.tostring(\"HH:mm\");\n",
    "#time = datetime.strptime(x, '%H:%M:%S')# - datetime.strptime(s1, format)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.254773\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'datetime.timedelta' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-502-72656703bbb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#for c in f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'datetime.timedelta' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "y = datetime.now()\n",
    "\n",
    "c = y-x\n",
    "f = str(c)\n",
    "print(f)\n",
    "f = f.split(\".\")[0]\n",
    "\n",
    "#for c in f:\n",
    "if c[0] == \"0\":\n",
    "    print(f[2:7])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## Prediction ############## \n",
      "\n",
      "Input Text:      Pazifische Taifunsaison 2015 Die Pazifische Taifunsaison 2015 ist ein andauerndes Wetterereignis, un\n",
      "\n",
      " predicting...   Pazifische Taifunsaison 2015 die Pazifische (predicted) Taifunsaison 2015 ist ein andauerndes Wetterereignis , unter dem die sich während des gesamten (predicted) Kalenderjahres bildenden (predicted) tropischen (predicted) Wirbelstürme zusammengefasst (predicted) sind . Die meisten (predicted) tropischen (predicted) Wirbelstürme bilden (predicted) sich im Pazifischen (predicted) Ozean westlich (predicted) der Datumsgrenze und nördlich (predicted) des Äquators jedoch (predicted) zwischen Mai (predicted) und November . Solche tropischen (predicted) Wirbelstürme nennt (predicted) man Taifune . Die sich östlich (predicted) des 180 . Längengrades bildenden (predicted) Stürme sind Gegenstand der Pazifischen (predicted) Hurrikansaison 2015 . Innerhalb des nordwestlichen (predicted) Pazifiks gibt (predicted) es zwei (predicted) Meteorologische (predicted) Organisationen (predicted) , die den Stürmen (predicted) Namen (predicted) vergeben (predicted) . Dies führt (predicted) dazu , dass derselbe Sturm (predicted) häufig (predicted) zwei (predicted) verschiedene (predicted) Namen (predicted) erhält (predicted) . Die offizielle (predicted) Bezeichnung (predicted) durch die Japan (predicted) Meteorological Agency (predicted) ( JMA ) erhält (predicted) \n",
      "\n",
      "Predicted Text:  Pazifische Taifunsaison 2015 die Pazifische Taifunsaison 2015 ist ein andauerndes Wetterereignis , u \n",
      "\n",
      "\n",
      "############## Evaluation ############## \n",
      "\n",
      "Total of  99  Words Predicted in  0:01:20 \n",
      "\n",
      "True Positive           ==>    41 \n",
      "False Positive          ==>     1 \n",
      "True Negative           ==>    56 \n",
      "False Negative          ==>     1 \n",
      "\n",
      "Accuracy: 97.9798 % \n",
      "\n",
      "False Positive Words: ['Meteorologische'] \n",
      "False Negative Words: ['die']\n"
     ]
    }
   ],
   "source": [
    "word_predictor(test_data, \"w2v_model_train_48.bin\", test_data, progress=\"show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "## beta v5 - working\n",
    "\n",
    "def word_predictor(str_input, model_filename, evaluation = None, progress = \"\"):\n",
    "    \n",
    "    \"\"\"  \n",
    "    str_input.........takes any German text as input, must be a single string\n",
    "    model_filename....takes a pretrained Word2Vec Embedding, will be loaded with Gensim\n",
    "    evaluation........optional, takes same text as in str_input but with correct capitalization\n",
    "    progress..........optional, use progress=\"show\" for displaying prediction progress\n",
    "    \"\"\"\n",
    "    \n",
    "    global str_output, predict_with, start_time, end_time\n",
    "    split_orig, split_low, split_cap, found, unknown, predict_output, predict_with = [], [], [], [], [], [], []\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(\" Prediction \".center(40, '#') ,\n",
    "            \"\\n\"\"\\n\" \n",
    "            \"Input Text:     \" , str_input[0:100])\n",
    "    print(\"\\n\" , \"predicting...\", end= \"   \")\n",
    "    \n",
    "    model = gensim.models.Word2Vec.load(model_filename)\n",
    "    words = list(model.wv.vocab)\n",
    "    \n",
    "    split_orig = re.findall(r\"\\w+|[^\\w\\s]\", str_input, re.UNICODE)\n",
    "    split_in_words = [x for x in split_orig if x in words]\n",
    "    split_low = [x.lower() for x in split_orig]\n",
    "    split_cap = [x.capitalize() for x in split_orig]   \n",
    "\n",
    "    l = 0\n",
    "    while l < (len(split_orig)):\n",
    "        for i in split_orig:\n",
    "            if i in string.punctuation:\n",
    "                predict_output.append(i)\n",
    "                if progress==\"show\":\n",
    "                    print(i, end=\" \")\n",
    "                l += 1\n",
    "            elif l == 0:\n",
    "                predict_output.append(i.capitalize())\n",
    "                if progress==\"show\":\n",
    "                    print(i.capitalize(), end= \" \")\n",
    "                l += 1\n",
    "            elif predict_output[-1] == \".\":\n",
    "                predict_output.append(i.capitalize())\n",
    "                if progress==\"show\":\n",
    "                    print(i.capitalize(), end= \" \")\n",
    "                l += 1\n",
    "            else:\n",
    "                if split_low[l] in words:\n",
    "                    if split_cap[l] in words:\n",
    "                        if len(found) > 0:\n",
    "                            if len(split_in_words) >= (l+4):\n",
    "                                predict_with = [] + found\n",
    "                                predict_with.append(split_in_words[l+1])\n",
    "                                predict_with.append(split_in_words[l+2])    \n",
    "                                predict_with.append(split_in_words[l+3])\n",
    "\n",
    "                                predict_next = model.predict_output_word(predict_with[0:len(predict_with)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                for k in predict_next:\n",
    "                                    if split_low[l] == k[0]:\n",
    "                                        word_low = k[0]\n",
    "                                        val_low = k[1]\n",
    "                                for k in predict_next:\n",
    "                                    if k[0] == split_cap[l]:\n",
    "                                        word_cap = k[0]\n",
    "                                        val_cap = k[1]                                                \n",
    "\n",
    "                                if val_low >= val_cap:\n",
    "                                    predict_output.append(i.lower())\n",
    "                                    found.append(i.lower())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1 \n",
    "                                elif val_low <= val_cap:\n",
    "                                    predict_output.append(i.capitalize())\n",
    "                                    found.append(i.capitalize())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1  \n",
    "                                else:                                                                         \n",
    "                                    predict_output.append(i.upper())\n",
    "                                    unknown.append(i.upper())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.upper(), end= \" \")\n",
    "                                    l += 1                                    \n",
    "\n",
    "                            elif len(split_in_words) >= (l+3):\n",
    "                                predict_with = [] + found\n",
    "                                predict_with.append(split_in_words[l+1])\n",
    "                                predict_with.append(split_in_words[l+2])    \n",
    "\n",
    "                                predict_next = model.predict_output_word(predict_with[0:len(predict_with)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                for k in predict_next:\n",
    "                                    if split_low[l] == k[0]:\n",
    "                                        word_low = k[0]\n",
    "                                        val_low = k[1]\n",
    "                                for k in predict_next:\n",
    "                                    if k[0] == split_cap[l]:\n",
    "                                        word_cap = k[0]\n",
    "                                        val_cap = k[1]                                                \n",
    "\n",
    "                                if val_low >= val_cap:\n",
    "                                    predict_output.append(i.lower())\n",
    "                                    found.append(i.lower())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1 \n",
    "                                elif val_low <= val_cap:\n",
    "                                    predict_output.append(i.capitalize())\n",
    "                                    found.append(i.capitalize())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1  \n",
    "                                else:                                                                         \n",
    "                                    predict_output.append(i.upper())\n",
    "                                    unknown.append(i.upper())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.upper(), end= \" \")\n",
    "                                    l += 1                                              \n",
    "\n",
    "                            elif len(split_in_words) > (l+1): # predict with l+1\n",
    "                                predict_with = [] + found\n",
    "                                predict_with.append(split_in_words[l+1])\n",
    "                                predict_next = model.predict_output_word(predict_with[0:len(predict_with)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                for k in predict_next:\n",
    "                                    if split_low[l] == k[0]:\n",
    "                                        word_low = k[0]\n",
    "                                        val_low = k[1]\n",
    "                                for k in predict_next:\n",
    "                                    if k[0] == split_cap[l]:\n",
    "                                        word_cap = k[0]\n",
    "                                        val_cap = k[1]                                                \n",
    "\n",
    "                                if val_low >= val_cap:\n",
    "                                    predict_output.append(i.lower())\n",
    "                                    found.append(i.lower())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1 \n",
    "                                elif val_low <= val_cap:\n",
    "                                    predict_output.append(i.capitalize())\n",
    "                                    found.append(i.capitalize())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1  \n",
    "                                else:                                                                         \n",
    "                                    predict_output.append(i.upper())\n",
    "                                    unknown.append(i.upper())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.upper(), end= \" \")\n",
    "                                    l += 1       \n",
    "\n",
    "                            else:\n",
    "                                if len(found) >= 10:\n",
    "                                    predict_next = model.predict_output_word(found[-10:len(found)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                    for k in predict_next:\n",
    "                                        if split_low[l] == k[0]:\n",
    "                                            word_low = k[0]\n",
    "                                            val_low = k[1]\n",
    "                                    for k in predict_next:\n",
    "                                        if split_cap[l] == k[0]:\n",
    "                                            word_cap = k[0]\n",
    "                                            val_cap = k[1]\n",
    "\n",
    "                                    if val_low >= val_cap:\n",
    "                                        predict_output.append(i.lower())\n",
    "                                        found.append(i.lower())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                        l += 1 \n",
    "                                    elif val_low <= val_cap:\n",
    "                                        predict_output.append(i.capitalize())\n",
    "                                        found.append(i.capitalize())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                        l += 1  \n",
    "                                    else:                                                                         \n",
    "                                        predict_output.append(i.upper())\n",
    "                                        unknown.append(i.upper())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.upper(), end= \" \")\n",
    "                                        l += 1\n",
    "\n",
    "                                elif len(found) >= 5:  \n",
    "                                    # predict only with the last 5\n",
    "                                    predict_next = model.predict_output_word(found[-5:len(found)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                    for k in predict_next:\n",
    "                                        if split_low[l] == k[0]:\n",
    "                                            word_low = k[0]\n",
    "                                            val_low = k[1]\n",
    "\n",
    "                                    for k in predict_next:\n",
    "                                        if split_cap[l] == k[0]:\n",
    "                                            word_cap = k[0]\n",
    "                                            val_cap = k[1]\n",
    "\n",
    "                                    if val_low >= val_cap:\n",
    "                                        predict_output.append(i.lower())\n",
    "                                        found.append(i.lower())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                        l += 1 \n",
    "                                    elif val_low <= val_cap:\n",
    "                                        predict_output.append(i.capitalize())\n",
    "                                        found.append(i.capitalize())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                        l += 1  \n",
    "                                    else:                                                                         \n",
    "                                        predict_output.append(i.upper())\n",
    "                                        unknown.append(i.upper())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.upper(), end= \" \")\n",
    "                                        l += 1    \n",
    "\n",
    "                                else:\n",
    "                                    predict_next = model.predict_output_word(found[0:len(found)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                    for k in predict_next:\n",
    "                                        if split_low[l] == k[0]:\n",
    "                                            word_low = k[0]\n",
    "                                            val_low = k[1]\n",
    "\n",
    "                                    for k in predict_next:\n",
    "                                        if split_cap[l] == k[0]:\n",
    "                                            word_cap = k[0]\n",
    "                                            val_cap = k[1]\n",
    "\n",
    "                                    if val_low >= val_cap:\n",
    "                                        predict_output.append(i.lower())\n",
    "                                        found.append(i.lower())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                        l += 1 \n",
    "                                    elif val_low <= val_cap:\n",
    "                                        predict_output.append(i.capitalize())\n",
    "                                        found.append(i.capitalize())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                        l += 1  \n",
    "                                    else:                                                                         \n",
    "                                        predict_output.append(i.upper())\n",
    "                                        unknown.append(i.upper())\n",
    "                                        if progress==\"show\":\n",
    "                                            print(i.upper(), end= \" \")\n",
    "                                        l += 1    \n",
    "\n",
    "                        else:\n",
    "                            if len(split_wo_punc) >= (l+4):\n",
    "                                predict_with = [] + found\n",
    "                                predict_with.append(split_in_words[l+1])\n",
    "                                predict_with.append(split_in_words[l+2])    \n",
    "                                predict_with.append(split_in_words[l+3])\n",
    "\n",
    "                                predict_next = model.predict_output_word(predict_with[0:len(predict_with)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                for k in predict_next:\n",
    "                                    if split_low[l] == k[0]:\n",
    "                                        word_low = k[0]\n",
    "                                        val_low = k[1]\n",
    "                                for k in predict_next:\n",
    "                                    if k[0] == split_cap[l]:\n",
    "                                        word_cap = k[0]\n",
    "                                        val_cap = k[1]                                                \n",
    "\n",
    "                                if val_low >= val_cap:\n",
    "                                    predict_output.append(i.lower())\n",
    "                                    found.append(i.lower())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1 \n",
    "                                elif val_low <= val_cap:\n",
    "                                    predict_output.append(i.capitalize())\n",
    "                                    found.append(i.capitalize())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1  \n",
    "                                else:                                                                         \n",
    "                                    predict_output.append(i.upper())\n",
    "                                    unknown.append(i.upper())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.upper(), end= \" \")\n",
    "                                    l += 1          \n",
    "\n",
    "                            elif len(split_wo_punc) >= (l+3):    \n",
    "                                predict_with = [] + found\n",
    "                                predict_with.append(split_in_words[l+1])\n",
    "                                predict_with.append(split_in_words[l+2])    \n",
    "\n",
    "                                predict_next = model.predict_output_word(predict_with[0:len(predict_with)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                for k in predict_next:\n",
    "                                    if split_low[l] == k[0]:\n",
    "                                        word_low = k[0]\n",
    "                                        val_low = k[1]\n",
    "                                for k in predict_next:\n",
    "                                    if k[0] == split_cap[l]:\n",
    "                                        word_cap = k[0]\n",
    "                                        val_cap = k[1]                                                \n",
    "\n",
    "                                if val_low >= val_cap:\n",
    "                                    predict_output.append(i.lower())\n",
    "                                    found.append(i.lower())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1 \n",
    "                                elif val_low <= val_cap:\n",
    "                                    predict_output.append(i.capitalize())\n",
    "                                    found.append(i.capitalize())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1  \n",
    "                                else:                                                                         \n",
    "                                    predict_output.append(i.upper())\n",
    "                                    unknown.append(i.upper())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.upper(), end= \" \")\n",
    "                                    l += 1                                                \n",
    "\n",
    "                            elif len(split_wo_punc) > (l+1): # predict with l+1                  \n",
    "                                predict_with = [] + found\n",
    "                                predict_with.append(split_in_words[l+1])  \n",
    "\n",
    "                                predict_next = model.predict_output_word(predict_with[0:len(predict_with)], topn=model.wv.vectors.shape[0]) \n",
    "\n",
    "                                for k in predict_next:\n",
    "                                    if split_low[l] == k[0]:\n",
    "                                        word_low = k[0]\n",
    "                                        val_low = k[1]\n",
    "\n",
    "                                for k in predict_next:\n",
    "                                    if k[0] == split_cap[l]:\n",
    "                                        word_cap = k[0]\n",
    "                                        val_cap = k[1]                                                \n",
    "\n",
    "                                if val_low >= val_cap:\n",
    "                                    predict_output.append(i.lower())\n",
    "                                    found.append(i.lower())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.lower(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1 \n",
    "                                elif val_low <= val_cap:\n",
    "                                    predict_output.append(i.capitalize())\n",
    "                                    found.append(i.capitalize())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.capitalize(), \"(predicted)\", end= \" \")\n",
    "                                    l += 1  \n",
    "                                else:                                                                         \n",
    "                                    predict_output.append(i.upper())\n",
    "                                    unknown.append(i.upper())\n",
    "                                    if progress==\"show\":\n",
    "                                        print(i.upper(), end= \" \")\n",
    "                                    l += 1                                               \n",
    "                            else:                                                                      \n",
    "                                predict_output.append(i.upper())\n",
    "                                unknown.append(i.upper())\n",
    "                                if progress==\"show\":\n",
    "                                    print(i.upper(), end= \" \")\n",
    "                                l += 1\n",
    "\n",
    "                    else:\n",
    "                        predict_output.append(i.lower())\n",
    "                        found.append(i.lower())\n",
    "                        if progress==\"show\":\n",
    "                            print(i.lower(), end= \" \")\n",
    "                        l += 1                           \n",
    "\n",
    "\n",
    "                elif split_cap[l] in words:\n",
    "                    predict_output.append(i.capitalize())\n",
    "                    found.append(i.capitalize())\n",
    "                    if progress==\"show\":\n",
    "                        print(i.capitalize(), end=\" \")\n",
    "                    l += 1\n",
    "                else: \n",
    "                    predict_output.append(i.upper())\n",
    "                    unknown.append(i.upper())\n",
    "                    if progress==\"show\":\n",
    "                        print(i.upper(), end= \" \")\n",
    "                    l += 1\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    str_output = \" \".join(predict_output)\n",
    "    str_output = re.sub(r'\\s([?:.!\"](?:\\s|$))', r'\\1', str_output)\n",
    "    str_output = str_output.replace('\" ', ' ')\n",
    "\n",
    "    if evaluation != None:\n",
    "        return evaluate(str_input, predict_output, evaluation)\n",
    "\n",
    "    words_predict = [x for x in input_x.split() if x not in string.punctuation]\n",
    "\n",
    "    print(\"\\n\"\"\\n\"\n",
    "          \"Predicted Text: \" , str_output[0:100] ,\n",
    "          \"\\n\"\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"t\"\n",
    "\n",
    "x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(str_input, predict_output, actual_output):\n",
    "    eval_predict, eval_act, evaluation, FP_words, FN_words = [], [], [], [], []\n",
    "    actual_output = re.findall(r\"\\w+|[^\\w\\s]\", actual_output, re.UNICODE)\n",
    "\n",
    "    time_elapsed = end_time - start_time\n",
    "    time_elapsed = str(time_elapsed)\n",
    "    time_elapsed = time_elapsed.split(\".\")[0] \n",
    "    \n",
    "    l = 0\n",
    "    while l < len(predict_output):\n",
    "        for i in predict_output:\n",
    "            if i in string.punctuation:\n",
    "                eval_predict.append(5)\n",
    "                l += 1\n",
    "            elif i == i.capitalize():\n",
    "                eval_predict.append(1)\n",
    "                l += 1\n",
    "            elif i == i.lower():\n",
    "                eval_predict.append(0)\n",
    "                l += 1\n",
    "            else:\n",
    "                eval_predict.append(5)\n",
    "                l += 1\n",
    "    l = 0\n",
    "    while l < len(actual_output):\n",
    "        for i in actual_output:\n",
    "            if i in string.punctuation:\n",
    "                eval_act.append(5)\n",
    "                l += 1\n",
    "            elif i == i.capitalize():\n",
    "                eval_act.append(1)\n",
    "                l += 1\n",
    "            elif i == i.lower():\n",
    "                eval_act.append(0)\n",
    "                l += 1\n",
    "            else:\n",
    "                eval_act.append(5)\n",
    "                l += 1  \n",
    "    l = 0            \n",
    "    while l < (len(eval_predict)):\n",
    "        for i in eval_predict:\n",
    "            if eval_predict[l] == 1 and eval_act[l] == 1:\n",
    "                evaluation.append(1)\n",
    "                l += 1\n",
    "            elif eval_predict[l] == 1 and eval_act[l] == 0:  \n",
    "                evaluation.append(2)\n",
    "                l += 1\n",
    "            elif eval_predict[l] == 0 and eval_act[l] == 0:\n",
    "                evaluation.append(3)\n",
    "                l += 1                        \n",
    "            elif eval_predict[l] == 0 and eval_act[l] == 1:\n",
    "                evaluation.append(4)\n",
    "                l += 1\n",
    "            elif eval_predict[l] == 5 and eval_act[l] == 5:\n",
    "                evaluation.append(5)\n",
    "                l += 1\n",
    "            else:\n",
    "                evaluation.append(6)\n",
    "                l += 1\n",
    "                \n",
    "    TP = evaluation.count(1)\n",
    "    FP = evaluation.count(2)\n",
    "    TN = evaluation.count(3)\n",
    "    FN = evaluation.count(4)\n",
    "    \n",
    "    T_total = TP+TN\n",
    "    F_total = FP+FN\n",
    "    total = TP+TN+FP+FN\n",
    "    \n",
    "    acc = (round(((T_total/total)*100), 4))\n",
    "    \n",
    "    index_FP = [int(i) for i,x in enumerate(evaluation) if x == 2]\n",
    "    index_FN = [int(i) for i,x in enumerate(evaluation) if x == 4]\n",
    "    \n",
    "    l = 0\n",
    "    while l < len(index_FP):\n",
    "        for i in index_FP:\n",
    "            FP_words.append(predict_output[i])\n",
    "            l += 1\n",
    "    l = 0        \n",
    "    while l < len(index_FN):\n",
    "        for i in index_FN:\n",
    "            FN_words.append(predict_output[i])\n",
    "            l += 1\n",
    "            \n",
    "    print(  \"\\n\"\"\\n\"\n",
    "            \"Predicted Text: \" , str_output[0:100] ,\n",
    "            \"\\n\"\"\\n\")\n",
    "    print(\" Evaluation \".center(40, '#') ,\n",
    "            \"\\n\"\"\\n\" \n",
    "            \"Total of \" , (TP+TN+FN+FP) , \" Words Predicted in \" , time_elapsed ,\n",
    "            \"\\n\"\"\\n\"    \n",
    "            f'{\"True Positive\":23} ==> {TP:5d}' ,\n",
    "            \"\\n\" \n",
    "            f'{\"False Positive\":23} ==> {FP:5d}' ,\n",
    "            \"\\n\"\n",
    "            f'{\"True Negative\":23} ==> {TN:5d}' ,\n",
    "            \"\\n\"\n",
    "            f'{\"False Negative\":23} ==> {FN:5d}' ,\n",
    "            \"\\n\"\"\\n\"  \n",
    "            \"Accuracy:\", acc , \"%\" ,\n",
    "            \"\\n\"\"\\n\"\n",
    "            \"False Positive Words:\", FP_words[0:10] ,\n",
    "            \"\\n\"\n",
    "            \"False Negative Words:\", FN_words[0:10] )     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result measurement - Accuracy\n",
    "\n",
    "#### 4 Test Data - Method Implementation - most_similar method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-0c5171d95a18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m print(\" Evaluation \".center(40, '#') ,\n\u001b[0;32m      9\u001b[0m       \u001b[1;34m\"\\n\\n\"\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m       \u001b[1;34mf'{\"Total Words Predicted\":20} ==> {(len(eval_predict)-eval_predict.count(5)):5d}'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m       \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m       \u001b[1;34mf'{\"True Positive\":20} ==> {TP:5d}'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eval_predict' is not defined"
     ]
    }
   ],
   "source": [
    "test = [1,2,3,4,2]\n",
    "\n",
    "#test[1][0]\n",
    "\n",
    "word = \"hello\"\n",
    "size = 10\n",
    "\n",
    "print(\" Evaluation \".center(40, '#') ,\n",
    "      \"\\n\\n\" ,\n",
    "      f'{\"Total Words Predicted\":20} ==> {(len(eval_predict)-eval_predict.count(5)):5d}' ,\n",
    "      f'{\"Prediction Time\":20} ==> {time_elapsed:5d}' ,\n",
    "      \"\\n\" \n",
    "      f'{\"True Positive\":20} ==> {TP:5d}' ,\n",
    "      f'{\"False Positive\":20} ==> {FP:5d}' ,\n",
    "      f'{\"True Negative\":20} ==> {TN:5d}' ,\n",
    "      f'{\"False Negative\":20} ==> {FN:5d}' ,\n",
    "      \"\\n\"\n",
    "      f'{\"Accuracy\":20} ==> {round((((TP+TN)/(TP+TN+FP+FN))*100), 2):5df}' ,\n",
    "      \"\\n\\n\"\n",
    "      f'{\"False Positive Words:\":20} ==> {FP_words[0:10]:5d}' ,\n",
    "      f'{\"False Negative Words:\":20} ==> {FN_words[0:10]:5d}' )                                                              \n",
    "                                                                     \n",
    "                                                                      \n",
    "                                                                      \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [int(i) for i,x in enumerate(test) if x == 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 0\n",
    "while l < len(x):\n",
    "    for i in x:\n",
    "        print(type(i))\n",
    "        FP_words = test[i]\n",
    "        l+=1\n",
    "FP_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result measurement - Comparison Accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
